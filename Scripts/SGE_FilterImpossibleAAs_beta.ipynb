{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from Bio import SeqIO\n",
    "from Bio.Seq import Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "oligo_sheet = '../Data/SNV_filtering_inputs/20240820_CURRENT_SGEoligos_2023.xlsx' #path to SGE oligos \n",
    "ref_path = '../Data/SNV_filtering_inputs/20240809_BARD1_SNVlib_ref_seqs_intron_annotated.xlsx' #path to annotated reference file (you will need to manually give coordinates for where exons are). I originally downloaded the sequence from benchling\n",
    "sge_scores = '../Data/20250821_BARD1scores_newmodel_wGMM.tsv'  #path to SGE datafile\n",
    "gene = 'BARD1' #name of your gene :)\n",
    "ref_sense = 0 #Sense of the reference file you provide\n",
    "filtered_file_name = '../Data/20250821_BARD1scores_newmodel_wGMM_FILTERED.xlsx' #name of saved file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "These first few functions are used to read files or used within other functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_unfiltered(sge_file): #read raw SGE data from excel\n",
    "    df = pd.read_csv(sge_file, sep = '\\t')\n",
    "    df['pos'] = df['pos'].astype(str)\n",
    "    df['pos_id'] = df['pos'] + ':' + df['alt']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ref(ref_file): #reads provided reference file\n",
    "    ref = pd.read_excel(ref_file)\n",
    "\n",
    "    return ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_oligo_sheet(file, gene): #reads the oligo spreadsheet and pulls sheet for specific gene\n",
    "    oligos_df = pd.read_excel(file, sheet_name = gene)\n",
    "    original_cols = oligos_df.columns.tolist()\n",
    "    \n",
    "    cleaned_cols = []\n",
    "\n",
    "    i = 0\n",
    "    while i < 3:\n",
    "        cleaned_cols.append(original_cols[i].upper())\n",
    "\n",
    "        i += 1\n",
    "        \n",
    "    oligos_df = oligos_df.rename(columns = {original_cols[0]: cleaned_cols[0],\n",
    "                                            original_cols[1]: cleaned_cols[1],\n",
    "                                            original_cols[2]: cleaned_cols[2]\n",
    "                                           }\n",
    "                                )\n",
    "    \n",
    "    oligos_df = oligos_df[['REGION','OLIGO NAME','PRIMER SEQUENCE']]\n",
    "    \n",
    "    return oligos_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_complement_string(seq_string): #Reverse complement and returns string\n",
    "    reverse_seq = seq_string[::-1]\n",
    "    reverse_comp_list = []\n",
    "    for char in reverse_seq:\n",
    "        if char == \"A\":\n",
    "            reverse_comp_list.append(\"T\")\n",
    "        elif char == \"G\":\n",
    "            reverse_comp_list.append(\"C\")\n",
    "        elif char == \"C\":\n",
    "            reverse_comp_list.append(\"G\")\n",
    "        else:\n",
    "            reverse_comp_list.append(\"A\")\n",
    "    reverse_compliment_str = \"\".join(reverse_comp_list)\n",
    "    return reverse_compliment_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_complement(seq_list): #Reverse complement and returns a list (retains upper/lowercase used in SGE oligo)\n",
    "    \n",
    "    i = 0\n",
    "    #while i < len(seq_list):\n",
    "        #seq_list[i] = seq_list[i].upper()\n",
    "        #i += 1\n",
    "    reverse_seq = seq_list[::-1]\n",
    "    reverse_comp_list = []\n",
    "    for char in reverse_seq:\n",
    "        if char == \"A\":\n",
    "            reverse_comp_list.append(\"T\")\n",
    "        elif char == 'T':\n",
    "            reverse_comp_list.append(\"A\")\n",
    "        elif char == \"G\":\n",
    "            reverse_comp_list.append(\"C\")\n",
    "        elif char == \"C\":\n",
    "            reverse_comp_list.append(\"G\")\n",
    "        elif char == 'a':\n",
    "            reverse_comp_list.append(\"t\")\n",
    "        elif char == 'c':\n",
    "            reverse_comp_list.append('g')\n",
    "        elif char == 'g':\n",
    "            reverse_comp_list.append('c')\n",
    "        else:\n",
    "            reverse_comp_list.append('a')\n",
    "            \n",
    "    reverse_compliment_str = \"\".join(reverse_comp_list)\n",
    "    return reverse_compliment_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutate_snvs(dna_sequence): #Mutates all possible SNVs of provided DNA sequence\n",
    "    snvs = []\n",
    "    i = 0\n",
    "    while i < len(dna_sequence):\n",
    "        if dna_sequence[i] == \"A\":\n",
    "            snvs.append(dna_sequence[:i] + \"T\" + dna_sequence[i + 1 :])\n",
    "            snvs.append(dna_sequence[:i] + \"C\" + dna_sequence[i + 1 :])\n",
    "            snvs.append(dna_sequence[:i] + \"G\" + dna_sequence[i + 1 :])\n",
    "        elif dna_sequence[i] == \"T\":\n",
    "            snvs.append(dna_sequence[:i] + \"A\" + dna_sequence[i + 1 :])\n",
    "            snvs.append(dna_sequence[:i] + \"C\" + dna_sequence[i + 1 :])\n",
    "            snvs.append(dna_sequence[:i] + \"G\" + dna_sequence[i + 1 :])\n",
    "        elif dna_sequence[i] == \"C\":\n",
    "            snvs.append(dna_sequence[:i] + \"A\" + dna_sequence[i + 1 :])\n",
    "            snvs.append(dna_sequence[:i] + \"T\" + dna_sequence[i + 1 :])\n",
    "            snvs.append(dna_sequence[:i] + \"G\" + dna_sequence[i + 1 :])\n",
    "        else:\n",
    "            snvs.append(dna_sequence[:i] + \"A\" + dna_sequence[i + 1 :])\n",
    "            snvs.append(dna_sequence[:i] + \"T\" + dna_sequence[i + 1 :])\n",
    "            snvs.append(dna_sequence[:i] + \"C\" + dna_sequence[i + 1 :])\n",
    "        i += 1\n",
    "    return snvs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "These functions below are used to process the oligos spreadsheet, get fixed edit locations, and get data needed for filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sgeoligos(df, gene, sense): #pulls SGE oligos from spreadsheet and makes reference seq like file\n",
    "    grouped = df.groupby(by = 'REGION') #grouops by region\n",
    "\n",
    "    if sense == 0:\n",
    "        sge_oligos = []\n",
    "        for target, group in grouped:\n",
    "            if len(group) > 0:\n",
    "                split_target = target.split(' ') #rewrites names of targets in oligo spreadsheet to be equal to that in SGE datafile\n",
    "                region = split_target[1]\n",
    "                target = gene + '_X'+ region\n",
    "    \n",
    "                oligo_name = target + '_sgeoligo'\n",
    "    \n",
    "                final_name = gene + '_X' + region.upper() #name equal to name in SGE datafile\n",
    "                \n",
    "    \n",
    "                #starts process to pull coordinates for SGE library\n",
    "                oligo_coords = group.loc[group['OLIGO NAME'].isin(['SNV library'])]\n",
    "                coords = str(oligo_coords['PRIMER SEQUENCE'])\n",
    "                row_split = coords.split(' ')\n",
    "                row_split = row_split[4]\n",
    "                coord_split = row_split.split('-')\n",
    "                chr_coord = coord_split[0]\n",
    "                \n",
    "                if chr_coord != 'dtype:': #gets SGE oligo start and end coordinates\n",
    "                    start_split = chr_coord.split(':')\n",
    "                    start = int(start_split[1])\n",
    "                    coord_name = coord_split[1]\n",
    "                    end_split = coord_name.split('\\n')\n",
    "                    end = int(end_split[0])\n",
    "                    \n",
    "                    oligo = group.loc[group['OLIGO NAME'].isin([oligo_name])] #gets SGE oligo\n",
    "                    oligo = oligo['PRIMER SEQUENCE'].tolist() #puts SGE oligo into list\n",
    "    \n",
    "    \n",
    "                    if len(oligo) > 0:\n",
    "                        oligo = oligo[0] #gets oligo string\n",
    "                        oligo = reverse_complement(oligo)\n",
    "                        coords = [] # list to hold coordinates\n",
    "                        region = [] # list to hold region name\n",
    "                        \n",
    "                        for i in range(start, end + 1): #creates list of coordinates and appends region name each time\n",
    "                            coords.append(i)\n",
    "                            region.append(final_name)\n",
    "                        #coords = coords[::-1] #flips coordinates for antisense gene\n",
    "        \n",
    "                        oligo_bp = [] #holds each bp in the library\n",
    "                        \n",
    "                        i = 0 \n",
    "                        while i < len(oligo): #iterates through string and appends letter\n",
    "                            oligo_bp.append(oligo[i])\n",
    "                            i += 1\n",
    "                            \n",
    "                        region_df = pd.DataFrame({'target': region, 'Reference': oligo_bp, 'pos': coords}) #creates region dataframe\n",
    "                    sge_oligos.append(region_df) #appends single region oligo dataframe to list\n",
    "                \n",
    "        final = pd.concat(sge_oligos) #concatenates all SGE oligo dataframes\n",
    "\n",
    "    elif sense == 1:\n",
    "        sge_oligos = []\n",
    "        for target, group in grouped:\n",
    "            if len(group) > 0:\n",
    "                split_target = target.split(' ') #rewrites names of targets in oligo spreadsheet to be equal to that in SGE datafile\n",
    "                region = split_target[1]\n",
    "                target = gene + '_X'+ region\n",
    "    \n",
    "                oligo_name = target + '_sgeoligo'\n",
    "    \n",
    "                final_name = gene + '_X' + region.upper() #name equal to name in SGE datafile\n",
    "                \n",
    "                #starts process to pull coordinates for SGE library\n",
    "                oligo_coords = group.loc[group['OLIGO NAME'].isin(['SNV library'])]\n",
    "                coords = str(oligo_coords['PRIMER SEQUENCE'])\n",
    "                row_split = coords.split(' ')\n",
    "                row_split = row_split[4]\n",
    "                coord_split = row_split.split('-')\n",
    "                chr_coord = coord_split[0]\n",
    "                \n",
    "                if chr_coord != 'dtype:': #gets SGE oligo start and end coordinates\n",
    "                    start_split = chr_coord.split(':')\n",
    "                    start = int(start_split[1])\n",
    "                    coord_name = coord_split[1]\n",
    "                    end_split = coord_name.split('\\n')\n",
    "                    end = int(end_split[0])\n",
    "                    \n",
    "                    oligo = group.loc[group['OLIGO NAME'].isin([oligo_name])] #gets SGE oligo\n",
    "                    oligo = oligo['PRIMER SEQUENCE'].tolist() #puts SGE oligo into list\n",
    "    \n",
    "    \n",
    "                    if len(oligo) > 0:\n",
    "                        oligo = oligo[0] #gets oligo string\n",
    "                        coords = [] # list to hold coordinates\n",
    "                        region = [] # list to hold region name\n",
    "                        \n",
    "                        for i in range(start, end + 1): #creates list of coordinates and appends region name each time\n",
    "                            coords.append(i)\n",
    "                            region.append(final_name)\n",
    "        \n",
    "                        oligo_bp = [] #holds each bp in the library\n",
    "                        \n",
    "                        i = 0 \n",
    "                        while i < len(oligo): #iterates through string and appends letter\n",
    "                            oligo_bp.append(oligo[i])\n",
    "                            i += 1\n",
    "                        #print(len(region), len(oligo_bp), len(coords)) #QC to check length of arrays\n",
    "                        region_df = pd.DataFrame({'target': region, 'Reference': oligo_bp, 'pos': coords}) #creates region dataframe\n",
    "                        \n",
    "                    sge_oligos.append(region_df) #appends single region oligo dataframe to list\n",
    "                \n",
    "        final = pd.concat(sge_oligos) #concatenates all SGE oligo dataframes\n",
    "\n",
    "\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_edits(oligos,gene): #gets fixed edits for that target\n",
    "    grouped = oligos.groupby(by = 'REGION') #groups dataframe by SGE region\n",
    "    edit_dicts = []\n",
    "    \n",
    "    for target, grouped in grouped:\n",
    "\n",
    "        split_target = target.split(' ') #rewrites names of targets in oligo spreadsheet to be equal to that in SGE datafile\n",
    "        region = split_target[1]\n",
    "        region = region.upper()\n",
    "        target = gene + '_X'+ region\n",
    "\n",
    "        edits = grouped.loc[grouped['OLIGO NAME'].isin(['Edits'])] #does string things to extract edits from spreadsheet \n",
    "        text_edit = str(edits['PRIMER SEQUENCE'])\n",
    "        colon_split_edit = text_edit.split(':') #splits on the shared colon\n",
    "        coord_edits = colon_split_edit[1] #Split string gives list, gets list component with coords of edits\n",
    "        comma_coord_split = coord_edits.split(',') #Edits are split by comma, splits the edits by the comma\n",
    "        \n",
    "        edit_list = [] #list to hold edits for this target\n",
    "        edit_dict = {} #placeholder dictionary to store target name and edit_list\n",
    "        for elem in comma_coord_split: #iterates through the list created by splitting the string containing the edits\n",
    "            chars = [] #list to hold each number\n",
    "            for char in elem: #loop checks to see if each character is a number\n",
    "                if char.isdigit():\n",
    "                    chars.append(char)\n",
    "                    \n",
    "            edit_coord = ''.join(chars) #joins list of characters to yield complete corrdinate\n",
    "            if len(edit_coord) > 1:\n",
    "                edit_list.append(int(edit_coord)) #appends coordinate to list\n",
    "                edit_dict[target] = edit_list #creates dictionary of edits for that target\n",
    "        if len(edit_dict) == 1:\n",
    "            edit_dicts.append(edit_dict) #appends target-specific dictionary to list containing edits for all regions\n",
    "    \n",
    "    return edit_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_filter_type(dicts,ref_path,sense): #generates dataframe that will be used to determine what kind of fixed edit is present\n",
    "\n",
    "    ref_all = pd.read_excel(ref_path) #reads reference file\n",
    "    \n",
    "    #lists for final dataframe\n",
    "    target_list = [] #holds target names\n",
    "    pos_list = [] #holds edit coordinates\n",
    "    ref_anti = [] #holds the antisense 4bp window\n",
    "    ref_sense = [] #holds the sense 4bp window\n",
    "    \n",
    "    for elem in dicts: \n",
    "        keys = elem.keys()\n",
    "        for key in keys:\n",
    "            target = key #stores name of SGE region \n",
    "            \n",
    "        ref_target = ref_all.loc[ref_all['target'].isin([target])] #pulls reference sequence for that SGE library  \n",
    "        ref_target = ref_target.loc[ref_target['Intron/Exon'].isin(['Exon'])] #excludes bases annotated as intronic to remove intronic HDR markers\n",
    "        \n",
    "        edits = elem[target] #gets list of edits\n",
    "\n",
    "        for elem in edits:\n",
    "                if sense == 0:\n",
    "                    target_ref_a = ref_target\n",
    "                    target_ref_s = ref_target\n",
    "                    min_2 = elem - 2\n",
    "    \n",
    "                    edit_coords_0 = []\n",
    "                    for i in range(min_2 + 1, elem + 3): #used to get 4bp window on antisense strand\n",
    "                        edit_coords_0.append(i)\n",
    "                    edit_coords_1 = []\n",
    "                    for i in range(min_2, elem + 2): #used to get 4 bp window on sense strand\n",
    "                        edit_coords_1.append(i)\n",
    "                            \n",
    "                    ref_0 = target_ref_a.loc[target_ref_a['pos'].isin(edit_coords_0)]\n",
    "                    ref_0 = ref_0['Reference'].to_list()\n",
    "                    ref_0 = reverse_complement(ref_0) #Gets 4 bp window that contains 2 bp upstream of edit and 1 bp downstream on antisense strand\n",
    "    \n",
    "                    ref_1 = target_ref_s.loc[target_ref_s['pos'].isin(edit_coords_1)]\n",
    "                    ref_1 = ref_1['Reference'].to_list()\n",
    "                    ref_1 = ''.join(ref_1) #Gets 4 bp window that contains 2 bp upstream of edit and 1 bp downstream on sense strand\n",
    "\n",
    "\n",
    "                    if len(ref_0) > 0: #tests to see if any bp are pulled (0 for intronic edits)\n",
    "                        target_list.append(target)\n",
    "                        pos_list.append(elem)\n",
    "                        ref_anti.append(ref_0)\n",
    "                        ref_sense.append(ref_1)\n",
    "                    \n",
    "                elif sense == 1:\n",
    "                    target_ref_a = ref_target\n",
    "                    target_ref_s = ref_target\n",
    "                    min_2 = elem - 2\n",
    "    \n",
    "                    edit_coords_0 = []\n",
    "                    for i in range(min_2 + 1, elem + 3): #used to get 4bp window on antisense strand\n",
    "                        edit_coords_0.append(i)\n",
    "                    edit_coords_1 = []\n",
    "                    for i in range(min_2, elem + 2): #used to get 4 bp window on sense strand\n",
    "                        edit_coords_1.append(i)\n",
    "                            \n",
    "                    ref_0 = target_ref_a.loc[target_ref_a['pos'].isin(edit_coords_0)]\n",
    "                    ref_0 = ref_0['Reference'].to_list()\n",
    "                    ref_0 = reverse_complement(ref_0) #Gets 4 bp window that contains 2 bp upstream of edit and 1 bp downstream on antisense strand\n",
    "    \n",
    "                    ref_1 = target_ref_s.loc[target_ref_s['pos'].isin(edit_coords_1)]\n",
    "                    ref_1 = ref_1['Reference'].to_list()\n",
    "                    ref_1 = ''.join(ref_1) #Gets 4 bp window that contains 2 bp upstream of edit and 1 bp downstream on sense strand\n",
    "\n",
    "\n",
    "                    if len(ref_0) > 0: #tests to see if any bp are pulled (0 for intronic edits)\n",
    "                        target_list.append(target)\n",
    "                        pos_list.append(elem)\n",
    "                        ref_anti.append(ref_0)\n",
    "                        ref_sense.append(ref_1)\n",
    "                    \n",
    "                \n",
    "\n",
    "    df = pd.DataFrame({'target': target_list, 'edit_coord': pos_list, 'anti': ref_anti, 'sense': ref_sense})\n",
    "\n",
    "    return df, ref_all\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def type_edits(df,ref, oligos, sense): #determines what type of edit is present in preparation for filtering. Needs prep_filter dataframe, ref seq dataframe, SGE oligos dataframe, and gene sense\n",
    "\n",
    "    #Lists to store characterized fixed edit tuples in form of (SGE region, coordinate of edit, bp of PAM site impacted, sense of PAM site, base change on sense strand)\n",
    "    pams = [] #list to store PAM edits (edits impacting NGG sites)\n",
    "    edits = [] #list to store other fixed edits (not at NGG sites, adjacent fixed edits under guide sequence)\n",
    "    same_codon_doubles = [] #list to store coordinates containing two fixed edits at the same codon\n",
    "    \n",
    "    grouped = df.groupby(by = 'target') #groups the dataframe created by the prep_filter_type function by SGE region\n",
    "    non_pam_coords  = [] #list to hold coordinates of fixed edits not at NGG sites\n",
    "    \n",
    "    for target, group in grouped: #iterates through each region and determines what fixed edits are present \n",
    "        target_edits = group['edit_coord'].tolist() #list of fixed edits for that SGE region\n",
    "        pam_df = group #group is a dataframe and is also assigned to this variable for future use in determining what PAMs are present\n",
    "        ref_target = ref.loc[ref['target'].isin([target])] #pulls out reference sequence for SGE region\n",
    "        target_oligo = oligos.loc[oligos['target'].isin([target])] #pulls out specific SGE oligo from concatenated dataframe \n",
    "        \n",
    "        i = 0\n",
    "        while (i + 1) < len(target_edits): #iterates through list of fixed edits and sorts all edits except for those at NGG PAM sites\n",
    "            test = target_edits[i + 1] - target_edits[i] #initial test to sort same codon edits and adjancent edits under guide\n",
    "\n",
    "            if test == 2: #Fixed edits within the same codon have difference between genomic coordinates of 2\n",
    "                tuple = (target, target_edits[i], target_edits[i + 1]) #creates tuple with (SGE region, 1st fixed edit, 2nd fixed edit)\n",
    "\n",
    "                #appends coordinates to non PAM site edits list\n",
    "                non_pam_coords.append(target_edits[i])\n",
    "                non_pam_coords.append(target_edits[i + 1])\n",
    "\n",
    "                #appends tuple to holding list for same codon fixed edits\n",
    "                same_codon_doubles.append(tuple)\n",
    "            \n",
    "            elif test == 3: #Adjacent fixed edits under the guide sequence will have difference between genomic coordinates of 3\n",
    "                edit_one = int(target_edits[i]) #Gets coordinate of first fixed edit\n",
    "                edit_two = int(target_edits[i + 1]) #Gets coordinate of second fixed edit\n",
    "\n",
    "                #lists to hold the coordinates of the adjacent codons impacted by fixed edits\n",
    "                codon_one_coords = []\n",
    "                codon_two_coords = []\n",
    "\n",
    "                #for loops used to create the genomic coordinates for each codon\n",
    "                for j in range(edit_one, (edit_one + 3)):\n",
    "                    codon_one_coords.append(j)\n",
    "                for j in range(edit_two, edit_two + 3):\n",
    "                    codon_two_coords.append(j)\n",
    "\n",
    "                #Gets the codons impacted by fixed edits from reference and the SGE oligo \n",
    "                codon1_df = ref_target.loc[ref_target['pos'].isin(codon_one_coords)]\n",
    "                codon2_df = ref_target.loc[ref_target['pos'].isin(codon_two_coords)]\n",
    "                codon1_oligo = target_oligo.loc[target_oligo['pos'].isin(codon_one_coords)]\n",
    "                codon2_oligo = target_oligo.loc[target_oligo['pos'].isin(codon_two_coords)]\n",
    "\n",
    "                #For both codons, dataframes are merged\n",
    "                codon1_merged = pd.merge(codon1_df,codon1_oligo, how = 'outer', on = ['target', 'Reference', 'pos'], indicator = True)\n",
    "                codon2_merged = pd.merge(codon2_df,codon2_oligo, how = 'outer', on = ['target', 'Reference', 'pos'], indicator = True)\n",
    "\n",
    "                #Dataframes are filtered to include only the base that is different (i.e. what the fixed edit is)\n",
    "                codon1_notshared = codon1_merged.loc[~codon1_merged['_merge'].isin(['both'])]\n",
    "                codon2_notshared = codon2_merged.loc[~codon2_merged['_merge'].isin(['both'])]\n",
    "\n",
    "                #Concatenates the dataframes for this pair of fixed edits\n",
    "                concat_codons = pd.concat([codon1_notshared,codon2_notshared])\n",
    "                concat_codons = concat_codons.drop(columns = ['Unnamed: 0','Intron/Exon', '_merge'])\n",
    "\n",
    "                #Groups fixed edits by genomic coordinate in preparation for tuple formation\n",
    "                grouped_codons = concat_codons.groupby( by = 'pos')\n",
    "\n",
    "                #iterates through group object and creates tuple for filtering \n",
    "                for pos, group in grouped_codons:\n",
    "                    name = group['target'][3]\n",
    "                    edit = group['Reference'][3]\n",
    "                    edit = edit.upper()\n",
    "                    pos = group['pos'][3]\n",
    "                    tuple = (name, pos,3,0, edit) #PAM position and sense component of this tuple not used and thus are fixed at 3 and 0\n",
    "                    non_pam_coords.append(pos)\n",
    "                    edits.append(tuple)\n",
    "\n",
    "\n",
    "            i += 1\n",
    "\n",
    "        #From pre-filtering dataframe, retrieves fixed eidts at PAM sites\n",
    "        pam_df = pam_df.loc[~pam_df['edit_coord'].isin(non_pam_coords)]\n",
    "        pam_df = pam_df.reset_index(drop = True) #resets index for while loop \n",
    "\n",
    "        #Sorts fixed PAM edits\n",
    "        if len(pam_df) > 0: #if statement needed to filter out empty dataframes generates from fixed edits not at PAM sites\n",
    "            i = 0\n",
    "            pattern = r\"[ACTG]GG\" #Regular expression for recognizing PAM sites\n",
    "\n",
    "            #iterates through dataframe row by row and sorts PAM site\n",
    "            while i < len(pam_df):\n",
    "                pos = pam_df['edit_coord'][i] #genomic coordinate of fixed edit\n",
    "                anti = (pam_df['anti'][i]).upper() #antisense 4bp window created from pre-filtering function\n",
    "                sense = (pam_df['sense'][i]).upper() #sense 4bp window created from pre-filtering function\n",
    "\n",
    "                if anti == 'GGGG' or sense == 'GGGG': #if any 4bp window contains GGGG, then it cannot be automatically filtered, user entry required\n",
    "                    print('Exception: GGGG PAM in ', target)\n",
    "                    pos_changed = int(input('PAM position changed: ')) #position in PAM that the fixed edit occurred (2 or 3)\n",
    "                    pam_sense = int(input('Sense of PAM: ')) #is the PAM on sense (1) or antisense (0) strand\n",
    "\n",
    "                    #if and elif statements determine what basechange occurs on the sense strand (used in SGE data output) occurs based on sense of PAM site\n",
    "                    if pam_sense == 0:\n",
    "                        base_change = 'G'\n",
    "                    elif pam_sense == 1:\n",
    "                        base_change = 'C'\n",
    "\n",
    "                    #creates and appends tuple with PAM information\n",
    "                    tuple = (target, pos, pos_changed, pam_sense, base_change)\n",
    "                    pams.append(tuple)\n",
    "\n",
    "                #Using regex, searches antisense windows for PAMs modified in the 3rd position on antisense strand\n",
    "                elif re.search(pattern, anti[0:3]):\n",
    "                    pos_changed = 3\n",
    "                    pam_sense = 0\n",
    "                    base_change = 'G' #base change is G as G->C on antisense strand leads to C->G change on sense strand\n",
    "                    tuple = (target, pos, pos_changed, pam_sense, base_change)\n",
    "                    pams.append(tuple)\n",
    "\n",
    "                #Using regex, searches antisense windows for PAMs modified in the 2nd position on antisense strand\n",
    "                elif re.search(pattern, anti[1:4]):\n",
    "                    pos_changed = 2\n",
    "                    pam_sense = 0\n",
    "                    base_change = 'G'\n",
    "                    tuple = (target, pos, pos_changed, pam_sense, base_change)\n",
    "                    pams.append(tuple)\n",
    "\n",
    "                #Using regex, searches sense windows for PAMs modified in the 3rd position on sense strand\n",
    "                elif re.search(pattern, sense[0:3]):\n",
    "                    pos_changed = 3\n",
    "                    pam_sense = 1\n",
    "                    base_change = 'C'\n",
    "                    tuple = (target, pos, pos_changed, pam_sense, base_change)\n",
    "                    pams.append(tuple)\n",
    "\n",
    "                #Using regex, searches sense windows for PAMs modified in the 2nd position on sense strand\n",
    "                elif re.search(pattern, sense[1:4]):\n",
    "                    pos_changed = 2\n",
    "                    pam_sense = 1\n",
    "                    base_change = 'C'\n",
    "                    tuple = (target, pos, pos_changed, pam_sense, base_change)\n",
    "                    pams.append(tuple)\n",
    "\n",
    "                i += 1\n",
    "                    \n",
    " \n",
    "    return pams, edits, same_codon_doubles\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "These functions are used for filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def remove_low_freq(df): #Removes all variants with low frequency either in SNV library or at Day 5 (Deprecated 4/14/25)\n",
    "    \n",
    "    #remove SNVs that were in library less than 1 in 10,0000\n",
    "    df_snv = df.loc[df['snvlib_freq'] > 0.001]\n",
    "\n",
    "    #remove variants present at day 5 with frequency less than 1 in 100,000\n",
    "    df_freq_1 = df_snv.loc[(df_snv['D05_R1R2R3_freq'] > 0.0001) & (df_snv['D05_R4R5R6_freq'] > 0.0001)]\n",
    "    #df_freq_2 = df_snv.loc[(df_snv['D05_R1R4_freq'] > 0.0001) & (df_snv['D05_R2R5_freq'] > 0.0001) & (df_snv['D05_R3R6_freq'] > 0.0001)]\n",
    "    df_freq_3 = df_snv.loc[(df_snv['D05_R1R4_freq'] > 0.0001) & (df_snv['D05_R2R5_freq'] > 0.0001)]\n",
    "\n",
    "    to_concat = [df_freq_1, df_freq_3]\n",
    "    df_freq = pd.concat(to_concat)\n",
    "    \n",
    "    return df_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pam_filter(df, pams, sense): #Iterates through fixed edits at NGG sites and removes new PAMs one bp away from original PAM as well as variants at PAM sites\n",
    "\n",
    "    for elem in pams: #iterates through list of PAM edits\n",
    "        var_to_filter = []\n",
    "        target, coord, pos, sense,fixed = elem\n",
    "\n",
    "        #filters out all variants at PAM edits\n",
    "        var_to_filter.append(str(coord) + ':G')\n",
    "        var_to_filter.append(str(coord) + ':C')\n",
    "        var_to_filter.append(str(coord) + ':A')\n",
    "        var_to_filter.append(str(coord) + ':T')\n",
    "\n",
    "        #Filters out new NGG PAMs one bp away from original PAM (possible with position 2 edit)\n",
    "        if pos == 2:\n",
    "            if sense == 1: #for PAM on positive sense strand\n",
    "                new_pam = coord +  2 #plus 2 for antisense gene\n",
    "                to_filter = str(new_pam) + ':G'\n",
    "                var_to_filter.append(to_filter)\n",
    "            if sense == 0: #for PAM on negative sense strand\n",
    "                new_pam = coord -  2 # -2 for antisense gene\n",
    "                to_filter = str(new_pam) + ':C' # ':C' as a N>C change on - sense will yield G on + sense strand\n",
    "                var_to_filter.append(to_filter)\n",
    "           \n",
    "        df = df.loc[~((df['pos_id'].isin(var_to_filter)) & (df['target'].isin([target])))] #filters out new PAM and PAM site edits present in the same SGE target\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_impossible_snvs(pam_filtered,all_fixed_edits,ref, gene_sense):\n",
    "    \n",
    "    #ref = ref.drop_duplicates(subset = ['pos'], keep = 'last') #drops duplicates from reference\n",
    "    impossible_snvs = []\n",
    "    impossible_snvs_df = []\n",
    "    \n",
    "    if gene_sense == 0:\n",
    "        for elem in all_fixed_edits: #gets list of possible amino acid changes\n",
    "            ref_mod = ref\n",
    "            target, coord, pos, sense, fixed = elem #unpacks edit tuple provided by user\n",
    "            ref_mod = ref_mod.loc[ref_mod['target'].isin([target])]\n",
    "            codon_coords = [] #stores coordinates for codon\n",
    "            \n",
    "            for i in range(coord, coord + 3): #creates coordinates for codon\n",
    "                codon_coords.append(i)\n",
    "\n",
    "            codon_coords = codon_coords[::-1] #Reverses order of coordinate list for antisense gene\n",
    "            \n",
    "            codon_ref = ref_mod.loc[ref_mod['pos'].isin(codon_coords)] #pulls out reference sequence from reference sequence dataframe\n",
    "            codon = codon_ref['Reference'].tolist() #turns reference sequence to list\n",
    "            codon = ''.join(codon) #joins references sequence to string\n",
    "            antisense_codon = reverse_complement_string(codon) #creates coding sequence\n",
    "\n",
    "            possible_snvs = mutate_snvs(antisense_codon) #generates all possible SNVs\n",
    "\n",
    "            if len(codon_ref) % 3 != 0 or len(codon_ref) != 3: #error checking - prints out outputs from previous that are not 1 codon only\n",
    "                print(codon_ref)\n",
    "                \n",
    "            #creates and stores all possible AA changes from reference sequence\n",
    "            possible_vars = []\n",
    "            for elem in possible_snvs:\n",
    "                var = Seq(elem)\n",
    "                aa = var.translate()\n",
    "                possible_vars.append(str(aa))\n",
    "                  \n",
    "            fixed_coords = []\n",
    "            fixed_vars = []\n",
    "            for i in range(coord + 1, coord + 3): #gets list of amino acid changes in context of fixed edits\n",
    "                fixed_coords.append(i)\n",
    "            rest_codon_fix = ref_mod.loc[ref_mod['pos'].isin(fixed_coords)] #gets bases in codon excluding fixed edit from reference dataframe\n",
    "\n",
    "            rest_codon_fix = rest_codon_fix['Reference'].tolist() #reference dataframe to list\n",
    "            codon_fix = [fixed] #fixed edit only\n",
    "            codon_fix += rest_codon_fix #full codon created\n",
    "            \n",
    "            codon_fix = ''.join(codon_fix)\n",
    "            fixed_coding = reverse_complement_string(codon_fix) #reverse complements for genes on negative sense strand\n",
    "            fixed_edit = fixed_coding[2] #gets fixed edit \n",
    "            to_mutate = fixed_coding[0:2] #gets not fixed edit\n",
    "            fixed_snvs = mutate_snvs(to_mutate) #creates all SNVs for not fixed edits\n",
    "            for elem in fixed_snvs: #generates all possible SNVs with the fixed edit\n",
    "                var = elem + fixed_edit\n",
    "                fixed_vars.append(var)\n",
    "\n",
    "            #Creates list of impossible variants (one list per fixed edit)\n",
    "            impossible = [] \n",
    "            j = 0\n",
    "            while j < len(fixed_vars):\n",
    "                fixed = Seq(fixed_vars[j]) #gets a variant with fixed edit\n",
    "                if len(fixed) != 3:\n",
    "                    print(fixed)\n",
    "                fixed_aa = str(fixed.translate()) #translates to amino acid\n",
    "                if fixed_aa in possible_vars:\n",
    "                    j +=1\n",
    "                else: #if amino acid isn't in the possible list, it is appended to the impossible list\n",
    "                    impossible.append(str(fixed))\n",
    "                    j +=1\n",
    "                    \n",
    "                \n",
    "            #print(impossible)\n",
    "            \n",
    "            #takes impossible SNVs and creates position IDs to filter out \n",
    "            \n",
    "            #takes unmutated, fixed edit codon, splits up by basepair\n",
    "            split_codon = [] #for ref column in final df that goes to list\n",
    "            k = 0\n",
    "            while k < len(fixed_coding):\n",
    "                split_codon.append(str(fixed_coding)[k])\n",
    "                k += 1\n",
    "\n",
    "            #iterates through each list of impossible SNVs to create position IDs for each list to store in df\n",
    "            for elem in impossible:\n",
    "                coords = codon_coords #coordinates of codon\n",
    "                ref_codon = split_codon #reference is the WT codon with fixed edit\n",
    "                fixed_codon = [] #list to hold basepairs of impossible codon\n",
    "                pos_id_list = [] #list to hold position IDs of impossible codons for this fixed edit\n",
    "\n",
    "                #splits up impossible codon by bp\n",
    "                i = 0\n",
    "                while i < len(elem):\n",
    "                    fixed_codon.append(elem[i])\n",
    "                    i += 1\n",
    "\n",
    "                #iterates through each basepair to determine where variant occurs and gets coordinate to create position ID\n",
    "                j = 0\n",
    "                while j < len(fixed_codon):\n",
    "                    if fixed_codon[j] != ref_codon[j]: #if statement determines mismatch\n",
    "                        coord = coords[j] #gets coordinate\n",
    "                        pos_id = str(coord) + ':' + reverse_complement(fixed_codon[j]) #creates position ID (reverse complement needed due to pos_ids being reported on sense strand)\n",
    "                        pos_id_list.append(pos_id)\n",
    "                    j += 1\n",
    "\n",
    "                #creates dictionary then dataframe with position IDs for this fixed edit\n",
    "                data = {'pos_id': pos_id_list} \n",
    "                target_names = []\n",
    "                for i in range(len(data)):\n",
    "                    target_names.append(target)\n",
    "                data['target'] = target_names   \n",
    "                df = pd.DataFrame(data)\n",
    "                \n",
    "                #print(df)\n",
    "                impossible_snvs.append(df)\n",
    "\n",
    "        impossible_snvs_df = pd.concat(impossible_snvs) #concatenates all impossible SNV dataframes\n",
    "\n",
    "    elif gene_sense == 1:\n",
    "        \n",
    "        for elem in all_fixed_edits: #gets list of possible amino acid changes\n",
    "            ref_mod = ref\n",
    "            target, coord, pos, sense, fixed = elem #unpacks edit tuple provided by user\n",
    "            ref_mod = ref_mod.loc[ref_mod['target'].isin([target])]\n",
    "            codon_coords = [] #stores coordinates for codon\n",
    "            for i in range(coord - 2, coord + 1): #creates coordinates for codon\n",
    "                codon_coords.append(i)\n",
    "            \n",
    "            codon_ref = ref_mod.loc[ref_mod['pos'].isin(codon_coords)] #pulls out reference sequence from reference sequence dataframe\n",
    "            codon = codon_ref['Reference'].tolist() #turns reference sequence to list\n",
    "            codon = ''.join(codon) #joins references sequence to string\n",
    "\n",
    "\n",
    "            possible_snvs = mutate_snvs(codon) #generates all possible SNVs\n",
    "\n",
    "            if len(codon_ref) % 3 != 0 or len(codon_ref) != 3: #error checking - prints out outputs from previous that are not 1 codon only\n",
    "                print(codon_ref)\n",
    "                \n",
    "            #creates and stores all possible AA changes from reference sequence\n",
    "            possible_vars = []\n",
    "            for elem in possible_snvs:\n",
    "                var = Seq(elem)\n",
    "                aa = var.translate()\n",
    "                possible_vars.append(str(aa))\n",
    "                \n",
    "            \n",
    "            fixed_coords = []\n",
    "            fixed_vars = []\n",
    "            for i in range(coord - 2, coord): #gets list of amino acid changes in context of fixed edits\n",
    "                fixed_coords.append(i)\n",
    "            rest_codon_fix = ref_mod.loc[ref_mod['pos'].isin(fixed_coords)] #gets bases in codon excluding fixed edit from reference dataframe\n",
    "\n",
    "            rest_codon_fix = rest_codon_fix['Reference'].tolist() #reference dataframe to list\n",
    "            codon_fix = [fixed] #fixed edit only\n",
    "            codon_fix = rest_codon_fix + codon_fix #full codon created\n",
    "            \n",
    "            fixed_coding = ''.join(codon_fix) #joins codon into one string\n",
    "\n",
    "            fixed_edit = fixed_coding[2] #gets fixed edit \n",
    "            to_mutate = fixed_coding[0:2] #gets not fixed edit\n",
    "\n",
    "            fixed_snvs = mutate_snvs(to_mutate) #creates all SNVs for not fixed edits\n",
    "            for elem in fixed_snvs: #generates all possible SNVs with the fixed edit\n",
    "                var = elem + fixed_edit\n",
    "                fixed_vars.append(var)\n",
    "                \n",
    "\n",
    "            #Creates list of impossible variants (one list per fixed edit)\n",
    "            impossible = [] \n",
    "            j = 0\n",
    "            while j < len(fixed_vars):\n",
    "                fixed = Seq(fixed_vars[j]) #gets a variant with fixed edit\n",
    "                if len(fixed) != 3:\n",
    "                    print(fixed)\n",
    "                fixed_aa = str(fixed.translate()) #translates to amino acid\n",
    "                if fixed_aa in possible_vars:\n",
    "                    j +=1\n",
    "                else: #if amino acid isn't in the possible list, it is appended to the impossible list\n",
    "                    impossible.append(str(fixed))\n",
    "                    j +=1\n",
    "                    \n",
    "                \n",
    "            #print(impossible)\n",
    "            \n",
    "            #takes impossible SNVs and creates position IDs to filter out \n",
    "            \n",
    "            #takes unmutated, fixed edit codon, splits up by basepair\n",
    "            split_codon = [] #for ref column in final df that goes to list\n",
    "            k = 0\n",
    "            while k < len(fixed_coding):\n",
    "                split_codon.append(str(fixed_coding)[k])\n",
    "                k += 1\n",
    "\n",
    "            #iterates through each list of impossible SNVs to create position IDs for each list to store in df\n",
    "            for elem in impossible:\n",
    "                coords = codon_coords #coordinates of codon\n",
    "                ref_codon = split_codon #reference is the WT codon with fixed edit\n",
    "                fixed_codon = [] #list to hold basepairs of impossible codon\n",
    "                pos_id_list = [] #list to hold position IDs of impossible codons for this fixed edit\n",
    "\n",
    "                #splits up impossible codon by bp\n",
    "                i = 0\n",
    "                while i < len(elem):\n",
    "                    fixed_codon.append(elem[i])\n",
    "                    i += 1\n",
    "\n",
    "                #iterates through each basepair to determine where variant occurs and gets coordinate to create position ID\n",
    "                j = 0\n",
    "                while j < len(fixed_codon):\n",
    "                    if fixed_codon[j] != ref_codon[j]: #if statement determines mismatch\n",
    "                        coord = coords[j] #gets coordinate\n",
    "                        pos_id = str(coord) + ':' + fixed_codon[j] #creates position ID (reverse complement needed due to pos_ids being reported on sense strand)\n",
    "                        pos_id_list.append(pos_id)\n",
    "                    j += 1\n",
    "\n",
    "                #creates dictionary then dataframe with position IDs for this fixed edit\n",
    "                data = {'pos_id': pos_id_list} \n",
    "                target_names = []\n",
    "                for i in range(len(data)):\n",
    "                    target_names.append(target)\n",
    "                data['target'] = target_names   \n",
    "                df = pd.DataFrame(data)\n",
    "            \n",
    "                impossible_snvs.append(df)\n",
    "\n",
    "        impossible_snvs_df = pd.concat(impossible_snvs) #concatenates all impossible SNV dataframes\n",
    "        \n",
    "    return impossible_snvs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_impossible_snvs(data_df, impossible_df): #Removes the impossible missense variants created due to context of fixed edits\n",
    "\n",
    "    grouped = impossible_df.groupby(by = 'target') #Groups dataframe created by previous function by SGE region\n",
    "\n",
    "    for target, group in grouped: #iterates through each group\n",
    "        to_remove = group['pos_id'].tolist() #each position ID to remove in that region is turned into list\n",
    "\n",
    "        data_df = data_df.loc[~((data_df['pos_id'].isin(to_remove)) & (data_df['target'].isin([target])))] #Datapoints with the same target and position ID are filtered out\n",
    "\n",
    "    data_filtered = data_df\n",
    "\n",
    "\n",
    "    return data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_doubles(df,doubles): #iterates through list of same codon double edits and filters out all variants within those codons\n",
    "\n",
    "    for elem in doubles: #iterates through list of same codon doubles\n",
    "        target, start, end = elem #unpacks tuple\n",
    "        double_coords = [] #list to hold codons with double edits\n",
    "        \n",
    "        for i in range(start, end + 1):\n",
    "            double_coords.append(i)\n",
    "        df = df.loc[~((df['pos'].isin(double_coords)) & (df['target'].isin([target])))] #removes double-edit codons\n",
    "    \n",
    "    #reindexes\n",
    "    df = df.reset_index(drop = True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(): #Runs all functions\n",
    "    \n",
    "    #Pre-filtering functions\n",
    "    oligos = read_oligo_sheet(oligo_sheet,gene)\n",
    "    sge_oligos = get_sgeoligos(oligos,gene,ref_sense)\n",
    "    edits = get_edits(oligos, gene)\n",
    "    type_ready, reference = prep_filter_type(edits,ref_path,ref_sense)\n",
    "    pams,edits, doubles = type_edits(type_ready, reference, sge_oligos, ref_sense)\n",
    "    all_fixed_edits = pams + edits\n",
    "    #displays fixed edits\n",
    "    print('Detected Edits follow: ', '\\n', \n",
    "          'PAM Edits: ',pams, '\\n', \n",
    "          'Other Fixed Edits: ', edits, '\\n',\n",
    "          'Codon Double Edits: ',doubles)\n",
    "    \n",
    "    #Functions for filtering\n",
    "    raw_data = read_unfiltered(sge_scores)\n",
    "    raw_data[\"gmm_consequence\"] = \"indeterminate\"\n",
    "    raw_data.loc[(raw_data[\"gmm_density_abnormal\"] >= 0.95), \"gmm_consequence\"] = \"functionally_abnormal\"\n",
    "    raw_data.loc[(raw_data[\"gmm_density_normal\"] >= 0.95), \"gmm_consequence\"] = \"functionally_normal\"\n",
    "    print(raw_data[\"gmm_consequence\"].value_counts())\n",
    "    ref = read_ref(ref_path)\n",
    "    pam_filtered = pam_filter(raw_data,pams, ref_sense)\n",
    "    impossible = get_impossible_snvs(pam_filtered, all_fixed_edits,ref,ref_sense)\n",
    "    snvs_filtered = filter_impossible_snvs(pam_filtered, impossible)\n",
    "    filtered_data = remove_doubles(snvs_filtered,doubles)\n",
    "\n",
    "    #SAVES FILE (Comment out if you don't want it!)\n",
    "    \n",
    "    \n",
    "    filtered_data.to_excel(filtered_file_name, index = False)\n",
    "\n",
    "    #Displays final filtered data and number of datapoints removed at each step\n",
    "    #print(filtered_data)\n",
    "    print('Filtering statistics: ', '\\n',\n",
    "          'RAW: ', len(raw_data), '\\n',\n",
    "          'POST PAM FILTER: ', len(pam_filtered), '\\n',\n",
    "          'POST SNV FILTER: ', len(snvs_filtered),'\\n',\n",
    "          'DONE: ', len(filtered_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
