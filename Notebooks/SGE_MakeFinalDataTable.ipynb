{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "This notebook builds all supplementary tables required for regenerating figures. A single, multi-tabbed excel file will be output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGE Score Files\n",
    "del_file = '../Data/supp_table_inputs/20251002_BARD1delscores.tsv' #BARD1 3bp deletion scores\n",
    "snv_file = '../Data/supp_table_inputs/20251002_BARD1snvscores.tsv' #BARD1 SNV scores\n",
    "thresholds = '../Data/supp_table_inputs/20251002_BARD1modelparams.tsv' #SGE pipeline output thresholds\n",
    "\n",
    "#ClinVar Files\n",
    "clinvar_snvs_file = '../Data/supp_table_inputs/20250912_BARD1_ClinVarSNVs_1StarPlus.txt' #ClinVar SNVs, at least 1 star review status, accessed 2025/09/12\n",
    "clinvar_dels_file = '../Data/supp_table_inputs/20250912_BARD1_ClinVarDels_1StarPlus.txt' #ClinVar Deletions, at least 1 star review status, accessed 2025/09/12\n",
    "\n",
    "#Allele Frequency Files\n",
    "gnom_path = '../Data/supp_table_inputs/20240905_BARD1_gnomADv4.1.0_SNVs.xlsx' #gnomAD v4.1.0 SNVs, accessed 2024/09/05\n",
    "reg_path = '../Data/supp_table_inputs/20240802_BARD1_Regeneron_MAF.xlsx' #Regeneron Million Exome allele frequency data. Accessed 2024/08/02\n",
    "\n",
    "#Normalized Depth Files\n",
    "read_depth_path = '../Data/supp_table_inputs/depth_data/depth_files'\n",
    "gsp_input_file = '../Data/supp_table_inputs/depth_data/deletion_inputs.xlsx'\n",
    "target_coords = '../Data/supp_table_inputs/20250415_BARD1_filter_entry.xlsx'\n",
    "cut_sites = '../Data/supp_table_inputs/20241217_BARD1_sgRNA_cutsites.xlsx'\n",
    "\n",
    "#Misc. Annotations\n",
    "snv_counts = '../Data/supp_table_inputs/20250825_BARD1counts.tsv' #Counts for assayed SNVs\n",
    "vep_predictions = '../Data/supp_table_inputs/20251002_BARD1snvs_VEP.xlsx' #VEP annotated file\n",
    "mutpred_prediction = '../Data/supp_table_inputs/20251006_BARD1_MutPred2.xlsx' #MutPred2 scores\n",
    "atg_scores = '../Data/supp_table_inputs/ATG_lib_data/20250409_BARD1_X1A_ATG_scored.xlsx' #ATG score file\n",
    "rna_scores = '../Data/supp_table_inputs/20250922_BARD1RNAscores.tsv' #RNA scores\n",
    "phylop = '../Data/supp_table_inputs/20251008_PhyloP.xlsx' #PhyloP scores\n",
    "edit_rate = '../Data/supp_table_inputs/20250926_BARD1.editrates.sorted.tsv' #Editing rates for useable reads file\n",
    "orthogonal_assays = '../Data/supp_table_inputs/20241016_Orthogonal_BARD1_FunctionalAssays.xlsx' #Curated orthogonal BARD1 assays\n",
    "alt.data_transformers.disable_max_rows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "Functions for initial read in of SGE data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thresholds(snv, thresholds): #Gets SGE thresholds\n",
    "    df = pd.read_csv(thresholds, sep = '\\t')\n",
    "\n",
    "    \n",
    "    thresholds = [df['thresh_abnormal'][0], df['thresh_normal'][0]]\n",
    "\n",
    "\n",
    "    df = pd.read_csv(snv, sep = '\\t')\n",
    "    #Some quick processing of SNV scores\n",
    "    df.loc[df['score'] >= 0, 'functional_consequence'] = 'functionally_normal' #Ensures that variants above our upper threshold (which is less than 0) will be assigned a functionally normal class\n",
    "    df['var_type'] = 'snv' #Sets variant type column to SNV\n",
    "    df['pos'] = df['pos'].astype(int)\n",
    "    df['start'] = df['pos']\n",
    "    df['end'] = df['pos']\n",
    "    df['pos_id'] = df['pos'].astype(str) + ':' + df['alt']\n",
    "    \n",
    "    return df, thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def class_dels(dels, thresholds):\n",
    "    dels = pd.read_csv(dels, sep = '\\t') #Reads deletions\n",
    "    \n",
    "    dels['var_type'] = '3bp_del'\n",
    "    dels['start'] = dels['pos'] + 1\n",
    "    dels['end'] = dels['pos'] + 3\n",
    "    dels['pos_id'] = dels['start'].astype(str) + \"-\" + dels['end'].astype(str)\n",
    "    \n",
    "    dels = dels.astype({'pos': int,\n",
    "                       'start': int,\n",
    "                       'end': int})\n",
    "    \n",
    "    return dels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "Function used to process and merge RNA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_rna(rna):\n",
    "    df = pd.read_csv(rna, sep = '\\t')\n",
    "    df = df.loc[~np.isinf(df['RNA_DNA_log2'])]\n",
    "\n",
    "\n",
    "    df = df[['pos_id', 'RNA_DNA_log2', 'exon', 'consequence']]\n",
    "\n",
    "    df = df.rename(columns = {'RNA_DNA_log2': 'RNAscore'})\n",
    "\n",
    "    df = df.groupby('pos_id').agg({\n",
    "        'RNAscore': 'mean',\n",
    "        'consequence': 'first',\n",
    "        'exon': 'first',\n",
    "    }).reset_index()\n",
    "\n",
    "\n",
    "    class_df = df.loc[df['consequence'].isin(['stop_gained'])]\n",
    "    class_df = class_df.copy()\n",
    "\n",
    "    class_df = class_df.loc[~class_df['exon'].isin(['BARD1_X1', 'BARD1_X2', 'BARD1_X11'])]\n",
    "    percentiles = class_df['RNAscore'].quantile(0.975)\n",
    "\n",
    "    class_df = class_df.loc[class_df['RNAscore'] <= percentiles]\n",
    "    \n",
    "\n",
    "    mean_stop = class_df['RNAscore'].mean()\n",
    "  \n",
    "    mean_std = class_df['RNAscore'].std()\n",
    "\n",
    "    lwrthresh = mean_stop  +  mean_std\n",
    " \n",
    "\n",
    "    df['RNA_consequence'] = 'normal'\n",
    "    df.loc[df['RNAscore'] <= lwrthresh, 'RNA_consequence'] = 'low'\n",
    "\n",
    "    df = df[['pos_id', 'RNAscore', 'RNA_consequence']]\n",
    "\n",
    "    return df, lwrthresh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "Functions used to process and merge the ClinVar data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_clinvar(snv_file, del_file): #Reads ClinVar data\n",
    "    \n",
    "    df = pd.read_csv(snv_file, delimiter='\\t') #reads ClinVar SNV tabular .txt \n",
    "    df = df[['Name','Protein change','GRCh38Chromosome','GRCh38Location','Germline classification']] #pulls useful columns\n",
    "    df = df.dropna(subset = ['GRCh38Location']) #Drops variants without genomic coordinate\n",
    "    df.GRCh38Location = df.GRCh38Location.astype(int) #Sets coordinates to integer data type\n",
    "    df['pos_id'] = None #preps for next function\n",
    "\n",
    "\n",
    "    del_df = pd.read_csv(del_file, sep = '\\t') #Reads ClinVar deletions\n",
    "    del_df = del_df.loc[del_df['GRCh38Location'].str.contains('-')] #Splits coordinates\n",
    "    del_df['start'] = del_df['GRCh38Location'].transform(lambda x: x.split(' - ')[0]) #Gets deletion start coordinate\n",
    "    del_df['end'] = del_df['GRCh38Location'].transform(lambda x: x.split(' - ')[1]) #Gets deletion end coordinate\n",
    "\n",
    "    #Sets coordinate data types to integer\n",
    "    del_df['start'] = del_df['start'].astype(int) \n",
    "    del_df['end'] = del_df['end'].astype(int)\n",
    "\n",
    "    del_df['del_length'] = del_df['end'] - del_df['start'] #Calculates deletion length \n",
    "\n",
    "    del_df = del_df.loc[del_df['del_length'].isin([2])] #Pulls out 3bp deletions\n",
    "    del_df['pos_id'] = del_df['start'].astype(str) + '-' + del_df['end'].astype(str) #Sets base change column to coordinate spanned by deletion\n",
    "    del_df = del_df[['pos_id', 'Germline classification']] #Pulls out necessary columns\n",
    "\n",
    "\n",
    "    return df, del_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair(base): #ClinVar gives base changes on negative sense strand, SGE pos_id on positive sense\n",
    "    if base == 'A':\n",
    "        return 'T'\n",
    "    elif base == 'T':\n",
    "        return 'A'\n",
    "    elif base == 'C':\n",
    "        return 'G'\n",
    "    else:\n",
    "        return 'C'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_base_changes(df): #Creates pos_id column in format of SGE datafile for ClinVar data    \n",
    "    k = 0\n",
    "    while k < len(df):\n",
    "        var = df['Name'][k]\n",
    "        coord = str(df['GRCh38Location'][k])\n",
    "        k += 1\n",
    "        i = 0\n",
    "        j = 3\n",
    "        while j < (len(var) + 1):\n",
    "            test_str = var[i:j]\n",
    "            j += 1\n",
    "            i += 1\n",
    "            sense_base = get_pair(test_str[2])\n",
    "            if test_str[1] == '>':\n",
    "                change = coord + \":\" + sense_base\n",
    "                df.loc[df['Name'] == var, 'pos_id'] = change\n",
    "                \n",
    "    df = df[['pos_id', 'Germline classification']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Functions used to merge allele frequency data from gnomAD and Regeneron Million Exomes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_gnomAD(gnomAD_path): #Reads gnomAD file\n",
    "    \n",
    "    unfiltered = pd.read_excel(gnomAD_path) #Reads gnomAD file\n",
    "    filtered = unfiltered[['gnomAD ID', 'Allele Frequency']] #Gets necessary columns \n",
    "\n",
    "    filtered = filtered.copy()\n",
    "    filtered['pos_id'] = filtered['gnomAD ID'].transform(lambda x: x[2:11] + ':' + x[14]) #Adds pos_id column for merging\n",
    "\n",
    "    filtered = filtered.rename(columns = {'Allele Frequency': 'gnomad_af'})\n",
    "    filtered = filtered[['pos_id', 'gnomad_af']]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_regeneron(reg_path): #Reads Regeneron data\n",
    "    \n",
    "    df = pd.read_excel(reg_path) #Reads data\n",
    "    maf = df[['Variant','AAF']] #Pulls necessary columns\n",
    "    maf = maf.copy()\n",
    "\n",
    "    maf = maf.rename(columns = {'AAF': 'regeneron_maf', 'Variant': 'pos_id'}) #Renames columns to share column names with SGE data\n",
    "\n",
    "    maf['pos_id'] = maf['pos_id'].transform(lambda x: x[2:12] + x[len(x) - 1: len(x) + 1]) #Remakes the pos_id column to match pos_id column from SGE data for merging\n",
    "    \n",
    "    return maf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Function to add SNV counts data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_counts(counts):\n",
    "    counts_df = pd.read_csv(counts, sep = '\\t')\n",
    "\n",
    "    counts_df['pos_id'] = counts_df['pos'].astype(str) + ':' + counts_df['alt']\n",
    "\n",
    "    return counts_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Fuction to add variant effect predictor data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_vep(file, mutpred_file):\n",
    "    vep_df = pd.read_excel(file)\n",
    "    mutpred_df = pd.read_excel(mutpred_file)\n",
    "\n",
    "    vep_df['pos'] = vep_df['Location'].transform(lambda x: x[-9:])\n",
    "    vep_df['pos_id'] = vep_df['pos'] + ':' + vep_df['Allele']\n",
    "    vep_df = vep_df.drop(columns = ['Location', 'Allele', 'pos'])\n",
    "    \n",
    "    vep_df['max_SpliceAI'] = vep_df[['SpliceAI_pred_DS_AG', 'SpliceAI_pred_DS_AL', 'SpliceAI_pred_DS_DG', 'SpliceAI_pred_DS_DL']].max(axis = 1)\n",
    "    vep_df = vep_df.rename(columns = {'SpliceAI_pred_DS_AG': 'SpliceAI_AG', \n",
    "                                      'SpliceAI_pred_DS_AL':'SpliceAI_AL',\n",
    "                                      'SpliceAI_pred_DS_DG': 'SpliceAI_DG',\n",
    "                                      'SpliceAI_pred_DS_DL': 'SpliceAI_DL',\n",
    "                                      'am_pathogenicity': 'am_score',\n",
    "                                      'CADD_PHRED': 'cadd_score',\n",
    "                                      'REVEL': 'revel_score'\n",
    "                                     })\n",
    "\n",
    "    mutpred_df = mutpred_df[['hg38_start', 'alt_allele', 'MutPred2']]\n",
    "    mutpred_df['pos_id'] = mutpred_df['hg38_start'].astype(str) + ':' + mutpred_df['alt_allele']\n",
    "    \n",
    "    mutpred_df = mutpred_df[['pos_id', 'MutPred2']]\n",
    "    vep_df = vep_df[['pos_id', 'SpliceAI_AG', 'SpliceAI_AL', 'SpliceAI_DG', 'SpliceAI_DL', 'max_SpliceAI', 'am_score', 'cadd_score', 'revel_score']]\n",
    "    vep_df = pd.merge(vep_df, mutpred_df, on = 'pos_id', how = 'left')\n",
    "    \n",
    "    return vep_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "Functions to add median normalized depth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input(file, cut_sites): #Reads input file containing coordinates for all exons\n",
    "    \n",
    "    input_params = pd.read_excel(file) #Reads input file\n",
    "\n",
    "    #Loop that creates list of genomic coordinates for coding sequence\n",
    "    i = 0\n",
    "    cds_coords = [] #List to hold coding coordinates\n",
    "    while i < len(input_params):\n",
    "        start = input_params['start'][i] #Gets starting coordinate\n",
    "        end = input_params['end'][i] #Gets end coordinate\n",
    "\n",
    "        #Makes coding coordinates\n",
    "        for j in range(start, end + 1):\n",
    "            cds_coords.append(j)\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "    cut_sites = pd.read_excel(cut_sites)\n",
    "    cut_sites.set_index('target', inplace = True)\n",
    "\n",
    "    return cds_coords, cut_sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_depth(depth_path, raw_sge,coding_coords, target_coords, cut_df): #Processes all depth files in directory and annotates them\n",
    "    \n",
    "    file_path = Path(depth_path) #Creates path object for depth files \n",
    "\n",
    "    depth_files = sorted(list(file_path.glob('*.tsv'))) #Gets all depth files\n",
    "    \n",
    "    columns = ['region', 'offset', 'depth'] #Column names for renaming depth files\n",
    "    \n",
    "    target_coordinates = pd.read_excel(target_coords, sheet_name = 'targets') #Reads input with SGE target coordinates\n",
    "    target_coordinates.set_index('target', inplace = True) #Sets target name to index\n",
    "\n",
    "    all_dfs = [] #Empty list to hold processed dataframes\n",
    "\n",
    "    #For loop iterates through all depth files and processes them\n",
    "    for file in depth_files:\n",
    "        df = pd.read_csv(file, sep = '\\t') #Reads depth file\n",
    "        df = df.set_axis(columns, axis = 1) #Renames columns \n",
    "\n",
    "        min_depth = df['depth'].min() #Gets minimum read count in file\n",
    "        max_depth = df['depth'].max() #Gets maximum read count in file\n",
    "\n",
    "        df['normdepth'] = df['depth'] / max_depth #Calculates normalized depth based on proportion of maximum read counts \n",
    "\n",
    "        full_region = df['region'][0] #Gets full SGE target\n",
    "        region_start = int(re.findall(r':(\\d+)-', full_region)[0]) #Gets starting coordinate for sequencing amplicon \n",
    "\n",
    "        df['pos'] = region_start + df['offset'] - 1 #Generates genomic coordinates for all regions based on offset column and starting coordinate\n",
    "\n",
    "        file_str = str(file) #Sets file name to string data type\n",
    "        region_rep = re.findall(r'/([^/]+)_D13\\.depth\\.tsv$', file_str)[0] #Gets region and replicate string\n",
    "\n",
    "        region_rep_split = region_rep.split('_') #Splits string on '_'\n",
    "        target = region_rep_split[0] + '_' + region_rep_split[1] #Gets target name\n",
    "        exon_test = region_rep_split[1][1:-1] #Gets exon \n",
    "\n",
    "        region_start = target_coordinates.loc[target, 'end'] #Gets SGE target starting coordinate\n",
    "        region_end = target_coordinates.loc[target, 'start'] #Gets SGE target end coordinate (end/start flipped due to antisense gene)\n",
    "    \n",
    "        region_coords = [] #List to hold coordinates in SGE target\n",
    "\n",
    "        for k in range(region_start, region_end + 1): #Loop creates coordinates for SGE target\n",
    "            region_coords.append(k)\n",
    "\n",
    "        #Booleans to get name of exon\n",
    "        if len(exon_test) > 0: #tests for all regions but X2\n",
    "            exon = exon_test\n",
    "        else: #Exception for X2\n",
    "            exon = '2'\n",
    "            \n",
    "        full_rep = region_rep_split[2] #Gets replicate value\n",
    "\n",
    "        #Boolean tests to assign replicate number\n",
    "        if full_rep == 'R1R4' or full_rep == 'R1R2R3':\n",
    "            rep = 'R1'\n",
    "        elif full_rep == 'R2R5' or full_rep == 'R4R5R6':\n",
    "            rep = 'R2'\n",
    "        elif full_rep == 'R3R6' or full_rep == 'R7R8R9':\n",
    "            rep = 'R3'\n",
    "\n",
    "        cut_site = cut_df.loc[target, 'pos']\n",
    "        \n",
    "        #Sets columns with identifying information\n",
    "        df['target'] = target\n",
    "        df['exon'] = exon\n",
    "        df['repl'] = rep\n",
    "        df['day'] = 'D13'\n",
    "        \n",
    "        df = df.loc[df['pos'].isin(region_coords)] #Dataframe filtered for coordinates in SGE target edited region only\n",
    "\n",
    "        df['cut_site_distance'] = -(df['pos'] - cut_site)\n",
    "         \n",
    "        all_dfs.append(df) #Dataframe appended to list\n",
    "    \n",
    "    final_df = pd.concat(all_dfs) #All dataframes concatenated\n",
    "    final_df = final_df.loc[final_df['pos'].isin(coding_coords)] #Dataframes filtered for coding sequencing only \n",
    "\n",
    "\n",
    "    raw_sge['amino_acid'] = raw_sge['amino_acid_change'].transform(lambda x: x[0:-1]) #Creates amino acid column \n",
    "    raw_sge = raw_sge.loc[~(raw_sge['amino_acid'].isin(['--']))] #Drops columns without amino acid \n",
    "    annotation_df = raw_sge[['pos', 'amino_acid']] #Keeps position column for mergining and amino acid column for annotation \n",
    "\n",
    "    final_df = pd.merge(final_df, annotation_df, on = 'pos', how = 'left') #Depth and annotation_df merged to annotate with amino acids\n",
    "   \n",
    "    \n",
    "\n",
    "    final_df['id'] = final_df['pos']  + final_df['depth']  + final_df['normdepth'] #Unique ID created for each datapoint \n",
    "\n",
    "    final_df = final_df.drop_duplicates(subset = 'id', keep = 'first') #Any duplicates dropped\n",
    "    final_df = final_df.drop(columns = ['id']) #ID column dropped \n",
    "\n",
    "    return final_df    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_read_depth(df): #Depth dataframe processed for visualization\n",
    "\n",
    "    df['pos'] = df['pos'].astype(str) #Sets 'pos' column to string datatype\n",
    "    df['target_id'] = df['target'] + ':' + df['pos'] #Builds a target ID column for target-based collapsing to median\n",
    "    \n",
    "    grouped = df.groupby('target_id') #Groups dataframe by target ID\n",
    "\n",
    "    #Creates dataframe with annotated CDS positions\n",
    "    cds_annotated = df.groupby('pos').agg({ #Grouping by position allows for accurate CDS pos. to be assigned\n",
    "    'normdepth': 'median', \n",
    "    'target': 'first',\n",
    "    'exon': 'first',\n",
    "    'amino_acid': 'last'\n",
    "    }).reset_index()\n",
    "\n",
    "    \n",
    "    cds_pos = [] #List to hold CDS position values\n",
    "\n",
    "    for i in range(len(cds_annotated)): #Builds CDS position values\n",
    "        cds_pos.append(i+1)\n",
    "\n",
    "    cds_pos = cds_pos[::-1] #Reverses values due to negative sense gene\n",
    "\n",
    "    \n",
    "    \n",
    "    cds_annotated['CDSpos'] = cds_pos #Adds CDS position column\n",
    "    cds_annotated = cds_annotated[['pos', 'CDSpos']] #Drops unncessary column s\n",
    "    \n",
    "    #Collapses depth calculations to median based on shared target and position for all 3 replicates\n",
    "    median_depth = grouped.agg({\n",
    "        'normdepth': 'median',\n",
    "        'pos': 'first',\n",
    "        'target': 'first',\n",
    "        'exon': 'first',\n",
    "        'amino_acid': 'first',\n",
    "        'cut_site_distance': 'first'\n",
    "    }\n",
    "                              )\n",
    "\n",
    "    median_depth = pd.merge(median_depth, cds_annotated, on = 'pos', how = 'left') #Merges with CDS annotated dataframe with add CDS position\n",
    "    median_depth = median_depth.rename(columns = {'normdepth': 'median_depth'}) #depth column renamed \n",
    "    \n",
    "    median_depth['AApos'] = median_depth['amino_acid'].str[1:]\n",
    "    #median_depth.to_excel('/Users/ivan/Desktop/test_excel_outputs/20250806_DepthOutput.xlsx', index = None)\n",
    "\n",
    "    aa_grouped = median_depth.groupby('AApos')\n",
    "\n",
    "    min_depth_aa_level = aa_grouped.agg({\n",
    "        'median_depth': 'min',\n",
    "        'target': 'first', \n",
    "        'exon': 'first',\n",
    "        'amino_acid': 'first',\n",
    "    }\n",
    "                                          )\n",
    "    min_depth_aa_level = min_depth_aa_level.reset_index(names = ['AApos'])\n",
    "    \n",
    "    return median_depth, min_depth_aa_level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "    #Reads SGE data\n",
    "    snv_df, snv_thresholds = get_thresholds(snv_file, thresholds)\n",
    "    del_df = class_dels(del_file, snv_thresholds)\n",
    "\n",
    "    sge_df = pd.concat([snv_df, del_df]) #Final concatenated SNVs and Deletions dataframe\n",
    "\n",
    "    #Reads and Merges RNA data\n",
    "    rna_df,rna_thresh = process_rna(rna_scores)\n",
    "\n",
    "    df = pd.merge(sge_df, rna_df, on = 'pos_id', how = 'left') #Merged with RNA scores and classifications\n",
    "\n",
    "\n",
    "    #Processes and Merges ClinVar Data\n",
    "    clinvar_snvs, clinvar_dels = read_clinvar(clinvar_snvs_file, clinvar_dels_file)\n",
    "    clinvar_snvs = get_base_changes(clinvar_snvs)\n",
    "    all_clinvar = pd.concat([clinvar_snvs, clinvar_dels])\n",
    "    \n",
    "    df = pd.merge(df, all_clinvar, on = 'pos_id', how = 'left') #df merged with ClinVar\n",
    "\n",
    "    #Processes and Merges MAF Data\n",
    "    gnomad_df = read_gnomAD(gnom_path)\n",
    "    regeneron_df = read_regeneron(reg_path)\n",
    "\n",
    "    df = pd.merge(df, gnomad_df, on = 'pos_id', how = 'left')\n",
    "    df = pd.merge(df, regeneron_df, on = 'pos_id', how = 'left')\n",
    "\n",
    "    #Merge in SNV counts\n",
    "    counts_df = read_counts(snv_counts)\n",
    "\n",
    "    #Merge PhyloP data\n",
    "    phylop_df = pd.read_excel(phylop)\n",
    "    phylop_df['pos'] = phylop_df['pos'].astype(int)\n",
    "    phylop_df = phylop_df.drop_duplicates(subset = ['phyloP'])\n",
    "    df = pd.merge(df, phylop_df, on = 'pos', how = 'left')\n",
    "\n",
    "    #Adds VEP data\n",
    "    vep_df = read_vep(vep_predictions, mutpred_prediction)\n",
    "\n",
    "    df = pd.merge(df, vep_df, on = 'pos_id', how = 'left')\n",
    "\n",
    "    #Calculates and adds normalized read depth for each position\n",
    "    coding_coords, cut_coords = read_input(gsp_input_file, cut_sites)\n",
    "    all_reps_depth = process_depth(read_depth_path, snv_df, coding_coords, target_coords, cut_coords)\n",
    "    collapsed_depth, min_collapsed_depth_aa = process_read_depth(all_reps_depth)\n",
    "\n",
    "\n",
    "    #Adds ATG data\n",
    "    atg_df = pd.read_excel(atg_scores)\n",
    "\n",
    "    \n",
    "    threshold_df = pd.DataFrame({'min': [snv_thresholds[0]], 'max': [snv_thresholds[1]], 'rna': [rna_thresh]})\n",
    "\n",
    "    #Adds edit rates\n",
    "    edit_df = pd.read_csv(edit_rate, sep = '\\t')\n",
    "\n",
    "    #Adds orthogonal assays\n",
    "    orthogonal_df = pd.read_excel(orthogonal_assays)\n",
    "\n",
    "    dfs = {'scores': df,\n",
    "           'snv_counts': counts_df,\n",
    "           'edit_rates': edit_df,\n",
    "           'thresholds': threshold_df,\n",
    "           'cutsites': cut_coords,\n",
    "           'median_pos_depth': collapsed_depth,\n",
    "           'min_aa_depth': min_collapsed_depth_aa,\n",
    "           'start_codon_scores': atg_df,\n",
    "           'orthogonal_data': orthogonal_df\n",
    "          }\n",
    "\n",
    "\n",
    "\n",
    "    print('Writing File...')\n",
    "\n",
    "    with pd.ExcelWriter('../Data/final_tables/BARD1_SGE_final_table.xlsx') as writer:\n",
    "        for sheet_name, df in dfs.items():\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "  \n",
    "    print('File Successfully Written')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
